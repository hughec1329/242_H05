\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\title{STA242 - HW04 - Working with Large files}
\author{Hugh Crockford}
\date{\today}
\begin{document}
	\maketitle
	\section{Using Shell tools}
		Using a combination of grep and uniq -c, I was able to get counts of flights leaving our 4 airports by scanning files by line, with no intermediate files. 
		I tried piping output of bzip -dc (-dc outputs directly to stdout) directly into my grep/uniq command, and this worked well for the individual sipped files, however when I tried this on the combined decade long files it tripped up during the uncompression and threw an error. I was able to run the exact same command with no issues on the uncompressed files, So I'm assuming the error had something to do with the bzip uncompression, maybe in between the multiple files?
		

		To get the average and standard deviation, I had to put code into a shell script and pass it arguments from R.
		I used awk for string processing, which was fast and is fairly easy to write.
		The awk statement was reasonably easy to work out once the nececary fields were identified, however I struggled when trying to generalise my statement, as the nested quotes ( double, single , and back) were  been evaluated before passed down to the shell.
		I tried various methods (sprintf, paste, escaping quotes) in R but was unable to achieve a general result and ended up hardcoding into my script which is hence not generalisable.
		Using a case statement I was able to use one script to compute all variables recquired - counts, mean, and SD.

	\section{Using R}
		\subsection{Using file connections}
				

		\subsection{Using Databases}

	\section{Using MySQL on Amazon EC2 instance.}
		I'd been playing around with an EC2
		after setting up user on mysql 5.5 serer, I was able to establish a connection using dbconnect() 
		After trying various ways to insert delays data into mysql (LOAD LOCAL INFILE, LOAD INFILE, BULK IMPORT etc.) I found it 
		had to use --local-infile option to allow local adding - come funky permisisons error.
		Used scp to transfer R script files, and wget to pull data.
		Using the EC2 instance through ssh was good practice using a tmux session and forced me to use command line (vim, mysql, top etc) tools for everything. It was also usefuil to use tmux so if ssh pipe was broken, I could recover the session without any loss of data. 

		creating index on origin sped up avg calc from 0.74sec to 0.12 sec, a 600\% increase
		MySQL also has a handy inbuilt function for Standard deviation, STD, which made this calculation easier although it did not affect the time taken.
		I tried to use entire dataset to compare speeds amoung db's but hit the space limit on my free usage ec2.
		Even just the 2008.csv blew up the MySQL I'd installed locally so found out I can get a free micro amazon DB, and loaded data from local machine to 

		kept getting process's killed, which after some googling looked like it was a memory issue on amazon, which auto kills hungry process.
	\section{MonetDB locally}
		I was intersted in the speed gains given my monetdb.
		The moetdb client didn't like the Year or Month variable, and threw an error relating to primary keys so I'd assume its a reserved word.
		To get the data into monetdb the header row had to be deleted, and all NA's deleted as they didnt match data types. I used tr to translate NA's to 0's, which satisfied both int and varchar recquirements. 
		It was much faster to import data into a local monetdb than cloud mysql, but this was obviously due to the data upload.
	\section{Speed}
		I set up monetDB and MySQL on my machine to test the speed of each.
		A plot of the time taken by each db for the same function is below.
		As can be seen from this plot, monetDb outperformed all other dbs over all tests.
		One strange thing in my speed benchmarking was the effect of indexing. Using SQlite indexing on origin sped up computations by 230 percent, and monet sped up ~240 percent, however using MySQL it nearly doubled the time taken.

		\begin{figure}[h!]
			\centering
			\includegraphics[scale=0.7]{bench.jpg}
			\caption{Comparison of time taken for various databases on same tasks}
		\end{figure}
	\section{Git}
		My github repo can be found at: https://github.com/hughec1329/

\newpage
	\section{CODE}
	\subsection{R}
		\lstinputlisting[breaklines=TRUE]{H04.R}
	\subsection{shell}
		\lstinputlisting[breaklines=TRUE]{plan.sh}
		\lstinputlisting[breaklines=TRUE]{planes.sh}
	\subsection{Amazon RDS and EC2}

\end{document}
